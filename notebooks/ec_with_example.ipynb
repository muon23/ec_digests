{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import llms\n",
    "\n",
    "\n",
    "llm = llms.of(\"gemini-2t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "example = \"\"\"\n",
    "1. 結論：\n",
    "● FY2Q25營收696.3億美元，季增6.2%/年增12.3%，優於財測中值/市場預期1.5%/1.0%，其中Azure強勁增長31%帶動FY2Q25智慧雲端營收年增19%至255億美元，Al相關功能貢獻13%增長，優於預期。展望後市，管理層指引智慧雲端營收將增長19-20%．Azure將增長 31-32%，AI貢獻成長來自AI產能提升，資本支出方面，預期FY3Q25/FY4Q25資本支出將和FY2Q25維持相同水準，隨Deepseek開源模型技術突破為AI產業注入了全新的動能。\n",
    "\n",
    "2. 重點摘要：\n",
    "● 以部門別來看，因Azure強勁增長31%帶動FY2Q25智慧雲端營收年增19%至255億美元，Non AI略不如預期主因AI排擠效應，而Al相關功能貢獻13%增長，相較 FY1Q25貢獻12%，成長1個百分點，優於預期。整體AI需求仍大於可用產能。\n",
    "● 展望FY3Q25，管理層指引智慧雲端營收將增長19-20%．Azure將增長 31-32%，AI貢獻成長來自AI產能提升，而Non AI方面將續受到AI排擠效應影響。\n",
    "● FY2Q25資本支出為149億美元，季增8%、年增50%，主要為Al相關投資，其中約 50%用於基礎建設需求，50%用於伺服器的CPU及GPU需求。\n",
    "● 管理層預期FY3Q25/FY4Q25資本支出將和FY2Q25維持相同水準，FY2026年資本支出將隨需求持續成長，然因FY2025高基期增長率將下滑。\n",
    "\n",
    "3. 法說內容：\n",
    "(1) 現況分析\n",
    "● FY2Q25營收696.3億美元，季增6.2%/年增12.3%，優於財測中值/市場預期1.5%/1.0%。其中生產力和商業流程部門營收年增14%；智慧雲端部門營收年增19%；個人運算部門年持平。\n",
    "● FY2Q25毛利率68.7%，季減0.7/年增0.3個百分點，優於財測中值/市場預期0.8/0.8個百分點，毛利率成長主因產品組合優化抵銷AI基礎設施建置影響。\n",
    "● FY2Q25營業利益316.5億元，季增3.6%/年增17.1%，優於優於財測中值/市場預期5.0%/4.8%，優於預期主因AI相關建置優化營運效率。\n",
    "● FY2Q25 EPS為3.23美元，季減2.1%/年增10.2%，優於優於財測中值/市場預期4.0%/2.5%。\n",
    "\n",
    "(2) 未來展望\n",
    "● 預估FY3Q25生產力和商業流程部門營收年增11-12%；智慧雲端部門營收年增19-20%；個人運算部門年減14%。\n",
    "● 預估FY3Q25銷貨成本將成長19-20%。\n",
    "● 預估FY3Q25稅率為18%。\n",
    "● 展望FY3Q25，管理層指引智慧雲端營收將增長19-20%．Azure將增長 31-32%，AI貢獻成長來自AI產能提升，而Non AI方面將續受到AI排擠效應影響。\n",
    "● AI產能供不應求狀況將持續至FY3Q25，然管理層預期FY2025年底，相關AI投資將滿足需求。\n",
    "● 管理層預期FY3Q25/FY4Q25資本支出將和FY2Q25維持相同水準，FY2026年資本支出將隨需求持續成長，然因FY2025高基期增長率將下滑。\n",
    "● 預期未來幾年多數PC將為Copilot PC。\n",
    "\n",
    "(3) 公司營運\n",
    "● FY2Q25雲端收入為409億美元，年增21%。\n",
    "● 以部門別來看，因Azure強勁增長31%帶動智慧雲端營收年增19%至255億美元，Non AI略不如預期主因AI排擠效應，而Al相關功能貢獻13%增長，相較 FY1Q25貢獻12%，成長1個百分點，優於預期。整體AI需求仍大於可用產能。\n",
    "● AI相關營收年增175%，優於預期。\n",
    "● FY2Q25資本支出為149億美元，季增8%、年增50%，主要為Al相關投資，其中約 50%用於基礎建設需求，50%用於伺服器的CPU及GPU需求。\n",
    "● 在基礎建設方面，近三年增加超過一倍資料中心產能。\n",
    "● AI Scaling Law在Pre-Training及推論領域均持續發展。\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "# Locations of the data sources\n",
    "#\n",
    "\n",
    "data_root = \"../data\"         # Directory to the data\n",
    "ec_dir = \"earning_calls\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "article_dir = os.path.join(data_root, ec_dir, \"nvda\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for file in os.listdir(article_dir):\n",
    "    if not file.endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    fd = open(os.path.join(article_dir, file), \"r\")\n",
    "    text += fd.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "\" Is that something customers are planning on building, or will these infrastructures remain fairly distinct? Well, we build very different things than ASICs in some ways, completely different in some areas we intersect. We're different in several ways. One, NVIDIA's architecture is general. You know, whether you've optimized for autoregressive models or diffusion-based models or vision-based models or multimodal models or text models, we're great at all of it. We're great at all of it because our software stack is so...\\n our architecture's flexible, our software stack's ecosystem is so rich that we're the initial target of most exciting innovations and algorithms. And so by definition, we're much, much more general than narrow. We're also really good from the end to end, from data processing, the curation of the training data, to the training of the data, of course, to reinforcement learning used in post-training, all the way to inference with test-time scaling. So we're general, we're end-to-end, and we're everywhere. And because we're not in just one cloud, we're in every cloud, we could be on-prem, we could be in a robot. Our architecture is much more accessible and a great target, initial target, for anybody who's starting up.\\n a new company, and so we're everywhere. And then the third thing I would say is that our performance and our rhythm is so incredibly fast. Remember that these data centers are always fixed in size. They're fixed in size or they're fixed in power. And if our performance per watt is anywhere from 2x to 4x to 8x, which is not unusual, it translates directly to revenues. And so if you have a 100-megawatt data center, if the performance or the throughput in that 100-megawatt or that gigawatt data center is 4 times or 8 times higher, your revenues for that gigawatt data center is 8 times higher. And the reason that is so different than data centers of the past is because AI factories are directly monetizable through its tokens generated. And so the token throughput.\\n of our architecture being so incredibly fast is just incredibly valuable to all of the companies that are building these things for revenue generation reasons and capturing the fast ROIs. And so I think the third reason is performance. And then the last thing that I would say is the software stack is incredibly hard. Building an ASIC is no different than what we do. We have to build a new architecture. And the ecosystem that sits on top of our architecture is 10 times more complex today than it was two years ago. And that's fairly obvious, because the amount of software that the world is building on top of our architecture is growing exponentially, and AI is advancing very quickly. So bringing that whole ecosystem on top of multiple chips is hard. And so I would say that those four reasons, and then finally, I will say this.\\n Just because a chip is designed doesn't mean it gets deployed. And you've seen this over and over again. There are a lot of chips that get built. But when the time comes, a business decision has to be made. And that business decision is about deploying a new engine, a new processor, into a limited AI factory in size, in power, and in time. And our technology is not only more advanced, more performant, it has much, much better software capability. And very importantly, our ability to deploy is lightning fast. And so these things are not for the faint of heart, as everybody knows now. And so there's a lot of different reasons why we do well, why we win.\\n We have a line of Ben Reitz with Malias Research. Please go ahead. Yeah, hi. Ben Reitz is here. Hey, thanks a lot for the question. Hey, Jensen, it's a geography-related question. You know, you did a great job explaining some of the demand underlying factors here on the strength, but U.S. was up about $5 billion or so sequentially, and I think, you know, there is a concern about whether U.S. can pick up the slack if there's regulations towards other geographies. And I was just wondering, as we go throughout the year, you know, if this kind of surge in the U.S. continues and it's going to be – whether that's okay, and if that, you know, underlies your growth rate, how can you keep growing so fast with this mixed shift towards the U.S.? Your guidance looks like China is probably up sequentially. So just wondering if you could go through that dynamic and maybe collect and weigh in. Thanks a lot.\\n China is approximately the same percentage as Q4 and as previous quarters. It's about half of what it was before the export control, but it's approximately the same percentage. With respect to geographies, the takeaway is that AI is software. It's modern software. It's incredible modern software, but it's modern software. And AI has gone mainstream. AI is used in delivery services everywhere, shopping services everywhere. If you were to buy a quarter of milk and it's delivered to you, AI was involved. And so almost everything that a consumer service provides, AI is at the core of it.\\n A student will use AI as a tutor. Health care services use AI. Financial services use AI. No fintech company will not use AI. Every fintech company will. Climate tech company use AI. Mineral discovery now uses AI. The number of every higher education, every university uses AI. And so I think it is fairly safe to say that AI has gone mainstream and that it's being integrated into every application. And our hope is that, of course, the technology continues to advance safely and advance in a helpful way to our society. And with that, I do believe that we're at the beginning of this new transition. And what I mean by that in the\\n beginning is remember behind us has been decades of data centers and decades of computers that have been built and they've been built for a world of hand-coding and general-purpose computing and CPUs and so on so forth. And going forward I think it's fairly safe to say that that world is going to be almost all software be infused with AI. All software and all services will be based on ultimately based on machine learning. The data flywheel is going to be part of improving software and services and that the future of computers will be accelerated. The future computers will be based on AI. And we're really two years into that journey and in modernizing computers that have taken decades to build out. So I'm fairly sure that we're in the beginning of this new era. And then lastly in no\\n technology has ever had the opportunity to address a larger part of the world's GDP than AI. No software tool ever has. And so this is now a software tool that can address a much larger part of the world's GDP more than any time in history. And so the way we think about growth and the way we think about whether something is big or small has to be in the context of that. And when you take a step back and look at it from that perspective, we're really just in the beginnings. Your next question comes from the line of Aaron Rakers with Wells Fargo. Please go ahead. Aaron, your line is open. Your next question comes from Mark Lupas with Evercore ISI.\\n Hi, this is Martin Papas, thanks for taking the question. I had a clarification and a question, Colette, for the clarification. Did you say that enterprise within the data center grew 2X year-on-year for the January quarter? And if so, would that make it faster going than the hyperscalers? And then, Jensen, for you, the question, hyperscalers are the biggest purchasers of your solutions, but they buy equipment for both internal and external workloads, external workloads being cloud services that enterprises use. So the question is, can you give us a sense of how that hyperscaler spend splits between that external workload and internal? And as these new AI workloads and applications come up, would you expect enterprises to become a larger part of that consumption mix and does that impact how you develop your...\\n service your ecosystem. Thank you. Sure. Thanks for the question regarding our enterprise business. Yes, it grew to X and very similar to what we were seeing with our large CSPs. Keep in mind, these are both important areas to understand. Working with the CSPs can be working on large language models, can be working on inference in their own work, but keep in mind, this is also where the enterprises are surfacing. Your enterprises are both with your CSPs as well as in terms of building on their own. They're both, correct, growing quite well. The CSPs are about half of our business. And the CSPs have internal consumption and external consumption, as you say. And we're using, of course, use for internal consumption. We work very closely with...\\n with all of them to optimize workloads that are internal to them, because they have a large infrastructure of NVIDIA gear that they could take advantage of. And the fact that we could be used for AI on the one hand, video processing on the other hand, data processing like Spark, we're fungible. And so the useful life of our, of our infrastructure is much better. If the useful life is much longer, then the TCO is also lower. And so the second part is, how do we see the growth of enterprise or not CSPs, if you will, going forward? And the answer is, I believe long-term, it is by far larger. And the reason for that is because if you look at the\\n industry today and what is not served by the computer industry is largely industrial. So let me give you an example. When we say enterprise, and let's say let's use a car company as an example because they make both soft things and hard things. And so in the case of a car company, the employees would be what we call enterprise and agentic AI and software planning systems and tools and we have some really exciting things to share with you guys at GTC. Those agentic systems are for employees to make employees more productive, to design, to market, to plan, to operate their company. That's agentic AI. On the other hand, the cars that they manufacture also need AI. They need an AI system that trains the cars, treats this entire giant fleet of cars. And you know today there's a billion cars on the road today.\\n on the road, someday there'll be a billion cars on the road and every single one of those cars will be robotic cars. And they'll all be collecting data and we'll be improving them using an AI factory. Whereas they have a car factory today, in the future they'll have a car factory and an AI factory. And then inside the car itself is a robotic system. And so as you can see, there are three computers involved. And there's the computer that helps the people. There's the computer that builds the AI for the machineries. It could be, of course, it could be a tractor. It could be a lawnmower. It could be a human or a robot that's being developed today. It could be a building. It could be a warehouse. These physical systems require a new type of AI we call physical AI. They can't just understand the meaning of words and languages, but they have to understand the meaning of the world. Friction and inertia, object permanence, and cause and effect.\\n and all of those type of things that are common sense to you and I, but AI has to go learn those physical effects. So we call that physical AI. That whole part of using agentic AI to revolutionize the way we work inside companies, that's just starting. This is now the beginning of the agentic AI era, and you hear a lot of people talking about it, and we've got some really great things going on. And then there's the physical AI after that, and then there are robotic systems after that. And so these three computers are all brand new, and my sense is that long-term, this will be by far the larger of them all, which kind of makes sense. You know, the world's GDP is represented by either heavy industries or industrials and companies that are providing for those. Thank you.\\n with Wells Fargo, please go ahead. Yeah, thanks for letting me back in. Jensen, I'm curious as we now approach the two-year anniversary of really the hopper inflection that you saw in 2023 and in Gen AI in general, and we think about the roadmap you have in front of us, how do you think about the infrastructure that's been deployed from a replacement cycle perspective and whether, you know, if it's, you know, GB300 or if it's the Rubin cycle where we start to see maybe some refresh opportunity? I'm just curious to how you look at that. Yeah, I appreciate it. First of all, people are still using Voltas and Pascals and Amperes. And the reason for that is because there are always things that, because CUDA is so programmable, you could use it, right? Well, one of the major use cases right now is data processing and data curation. You find a circumstance?\\n that an AI model is not very good at. You present that circumstance to a vision language model, let's say. Let's say it's a car. You present that circumstance to a vision language model. The vision language model actually looks at the circumstances and says, this is what happened, and I wasn't very good at it. You then take that response, the prompt, and you go and prompt an AI model to go find in your whole lake of data other circumstances like that, whatever that circumstance was. And then you use an AI to do domain randomization and generate a whole bunch of other examples. And then from that, you can go train the model. And so you could use the Amperes to go and do data processing and data curation and machine learning-based search. And then you create the training.\\n training data set, which you then present to your Hopper systems for training. And so each one of these architectures are completely, you know, they're all CUDA compatible, and so everything runs on everything. But if you have infrastructure in place, then you can put the less intensive workloads onto the install base of the All of our CPUs are very well deployed. We have time for one more question, and that question comes from Atif Malik with Citi. Please go ahead. Hi, thank you for taking my question. I have a follow-up question, gross margins for Colette. Colette, I understand there are many moving parts, the Backwell Yields and Relink 72 and Ethernet mix, and you kind of tiptoed the earlier question if April quarter is the bottom, but second half would have to ramp like 200 basis points per quarter to get to the mid-70s.\\n range that you're giving for the end of the fiscal year, and we still don't know much about tariffs' impact to broader semiconductors. So what kind of gives you the confidence in that trajectory in the back half of this year? Yeah. Thanks for the question. Our gross margins, they're quite complex in terms of the material and everything that we've put together in a Blackwell system. Tremendous amount of opportunity to look at a lot of different pieces of that on how we can better improve our gross margins over time. Remember, we have many different configurations as well on Blackwell that will be able to help us do that. So together, working after we get some of these really strong ramping completed for our customers, we can begin a lot of that work. If not, we're going to probably start as soon as possible if we can, and if we can improve it in the short term\\n we will also do that. Tariffs, at this point, it's a little bit of an unknown. It's an unknown until we understand further what the U.S. government's plan is, both its timing, its where, and how much. So, at this time, we are awaiting. But, again, we would, of course, always follow export controls and or tariffs in that manner. Ladies and gentlemen, that does conclude our question-and-answer session. I'm sorry. Thank you. No, no. We're going to open up to Jensen. I just wanted to thank you. I just wanted to thank you. Thank you, Collette. The demand for Blackwell is extraordinary. AI is evolving beyond perception and generative AI into reasoning. With reasoning AI, we're observing another scaling law. In first...\\n time or test time scaling, the more computation, the more the model thinks, the smarter the answer. Models like OpenAI, ROC3, DeepSeq R1 are reasoning models that apply inference time scaling. Reasoning models can consume 100 times more compute. Future reasoning models can consume much more compute. DeepSeq R1 has ignited global enthusiasm. It's an excellent innovation. But even more importantly, it has open-sourced a world-class reasoning AI model. Nearly every AI developer is applying R1, or chain of thought and reinforcement learning techniques like R1, to scale their model's performance. We now have three scaling laws, as I mentioned earlier, driving the demand for AI computing. The traditional scaling laws of AI remains intact.\\n Foundation models are being enhanced with multi-modality, and pre-training is still growing. But it's no longer enough. We have two additional scaling dimensions. Post-training scaling, where reinforcement learning, fine-tuning, model distillation, require orders of magnitude more compute than pre-training alone. Inference time scaling and reasoning, where a single query can demand 100 times more compute. We designed Blackwell for this moment, a single platform that can easily transition from pre-training, post-training, and test-time scaling. Blackwell's FP4 transformer engine and NVLink72 scale-up fabric and new software technologies let Blackwell process reasoning AI models 25 times faster than Hopper.\\n Blackwell, in all of its configurations, is in full production. Each Grace Blackwell NVLink72 rack is an engineering marvel. One and a half million components produced across 350 manufacturing sites by nearly 100,000 factory operators. AI is advancing at light speed. We're at the beginning of reasoning AI and inference time scaling. But we're just at the start of the age of AI. Multimodal AI, enterprise AI, sovereign AI, and physical AI are right around the corner. We will grow strongly in 2025. Going forward, data centers will dedicate most of CapEx to accelerated computing and AI. Data centers will increasingly become AI factories. And every company will have either rented or self-operated.\\n I want to thank all of you for joining us today. Come join us at GTC in a couple of weeks.have been placed on mute to prevent any background noise. After the speaker's remarks, there will be a question and answer session. If you would like to ask a question during this time, simply press star followed by the number one on your telephone keypad. And if you'd like to withdraw your question, again, press star one. Thank you. Stuart Stecker, you may begin your conference. Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the fourth quarter of fiscal 2025. With me today from NVIDIA are Jensen Wong, President and Chief Executive Officer, and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the first quarter of fiscal 2026. The content of today's call is NVIDIA's property.\\n it can't be reproduced or transcribed without prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release. Our most recent forms, 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made, as of today, February 26, 2025, based on information currently available to us, except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures and find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on\\n on our website. With that, let me turn the call over to Collette. Thanks, Stuart. Q4 was another record quarter. Revenue of 39.3 billion was up 12% sequentially and up 78% year on year. And above our outlook of 37.5 billion. For fiscal 2025, revenue was 130.5 billion, up 114% in the prior year. Let's start with data center. Data center revenue for fiscal 2025 was 115.2 billion, more than doubling from the prior year. In the fourth quarter, data center revenue of 35.6 billion was a record, up 16% sequentially and 93% year on year. As the Blackwell ramp commenced at Hopper 200 continued sequential growth.\\n In Q4, Blackwell sales exceeded our expectations. We delivered $11 billion of Blackwell revenue to meet strong demand. This is the fastest product ramp in our company's history, unprecedented in its speed and scale. Blackwell production is in full gear across multiple configurations, and we are increasing supply quickly, expanding customer adoption. Our Q4 data center compute revenue jumped 18% sequentially and over 2x year on year. Customers are racing to scale infrastructure to train the next generation of cutting-edge models and unlock the next level of AI capabilities. With Blackwell, it will be common for these clusters to start with 100,000 GPUs or more. Shipments have already started for multiple infrastructures of this size.\\n Post-training and model customization are fueling demand for NVIDIA infrastructure and software as developers and enterprises leverage techniques such as fine-tuning, reinforcement learning, and distillation to tailor models for domain-specific use cases. Hugging Face alone hosts over 90,000 derivatives graded from the Lama Foundation model. The scale of post-training and model customization is massive and can collectively demand orders of magnitude more compute than pre-training. Our inference demand is accelerating, driven by test-time scaling and new reasoning models like OpenAI's O3, DeepSeq R1, and Grok3. Long-thinking reasoning AI can require 100x more compute per task compared to one shot.\\n inferences. Blackwell was architected for reasoning AI inference. Blackwell supercharges reasoning AI models with up to 25X higher token throughput and 20X lower cost versus Hopper 100. It is revolutionary. Transformer Engine is built for LLM and mixture of experts inference. And its NVLink domain delivers 14X the throughput of PCIe Gen 5, ensuring the response time, throughput, and cost efficiency needed to tackle the growing complexity of inference of scale. Companies across industries are tapping into NVIDIA's full stack inference platform to boost performance and slash costs. Now tripled inference throughput and cut costs by 66% using NVIDIA TensorRT.\\n for its screenshot feature. Perplexity sees 435 million monthly queries and reduced its inference costs 3x with NVIDIA Triton Inference Server and TensorRT LLM. Microsoft Bing achieved a 5x speedup and major TCO savings for visual search across billions of images with NVIDIA TensorRT and acceleration libraries. Blackwell has great demand for inference. Many of the early GB200 deployments are earmarked for inference, a first for a new architecture. Blackwell addresses the entire AI market from pre-training, post-training, to inference across clouds, to on-premise, to enterprise. Sudha's programmable architecture accelerates every AI model and over 4,400 applications, ensuring large...\\n infrastructure investments against obsolescence in rapidly evolving markets. Our performance and pace of innovation is unmatched. We're driven to a 200x reduction in inference cost in just the last two years. We deliver the lowest TCO and the highest ROI. And full-stack optimizations for NVIDIA and our large ecosystem, including 5.9 million developers, continuously improve our customers' economics. In Q4, large CSPs represented about half of our data center revenue, and these sales increased nearly 2x year-on-year. Large CSPs were some of the first to stand up Blackwell with Azure, GCP, AWS, and OCI, bringing GV200 systems to cloud regions around the world to meet...\\n surging cluster demand for AI. Regional cloud hosting and video GPUs increased as a percentage of data center revenue, reflecting continued AI factory build-outs globally and rapidly rising demand for AI reasoning models and agents. Or we've launched a 100,000 GB 200 cluster-based instance with NVLink Switch and Quantum2 InfiniBand. Consumer Internet Revenue Group 3X year on year, driven by an expanding set of generative AI and deep learning use cases. These include recommender systems, vision language understanding, synthetic data generation search and agentic AI. For example, XAI is adopting the GB 200 to train and inference its next generation of grog AI models. Meta's cutting-edge and dry.\\n Adromeda advertising engine runs on NVIDIA's Grace Hopper superchip, serving vast quantities of ads across Instagram, Facebook applications. Adromeda harnesses Grace Hopper's fast interconnect and large memory to boost inference throughput by 3x, enhance ad personalization, and deliver meaningful jumps in monetization and ROI. Increase nearly 2x year-on-accelerated demand for model fine-tuning, RAG, and agentic AI workflows, and GPU-accelerated data processing. We introduced NVIDIA LLAMA Numitron model family NIMS to help developers create and deploy AI agents across a range of applications, including customer support, fraud detection, product supply chain, and inventory management.\\n AI agent platform providers, including SAP and ServiceNow, are among the first to use new models. Healthcare leaders IQVIA, Illumina, and Mayo Clinic are well, as AHRQ Institute, are using NVIDIA AI to speed drug discovery, enhance genomic research, and pioneer advanced healthcare services with generative and agentic AI. As AI expands beyond the digital world, NVIDIA infrastructure and software platforms are increasingly being adopted to power robotics and physical AI development. One of the early and largest robotics applications in autonomous vehicles, where virtually every AV company is developing on NVIDIA in the data center, the car, or both. NVIDIA's automotive vertical revenue is expected to grow to approximately five billion this fiscal year.\\n At CES, Hyundai Motor Group announced it is adopting NVIDIA technologies to accelerate AV and robotics development at smart factory initiatives. Vision transformers, self-supervised learning, multimodal sensor fusion, and high-fidelity simulation are driving breakthroughs in AV development and will require 10X more compute. At CEX, we announced the NVIDIA COSMOS World Foundation Model Platform. Just as language foundation models have revolutionized language AI, COSMOS is a physical AI to revolutionize robotics. Medium robotics and automotive companies, including ride-sharing giant Uber, are among the first to adopt the platform. From a geographic perspective, sequential growth in our data center revenue was strongest in the U.S.\\n driven by the initial ramp of Blackwell. Countries across the globe are building their AI ecosystems and demand for compute infrastructure is surging. France's 200 billion euro AI investment and the EU's 200 billion euro invest AI initiatives offer a glimpse into the build out to set redefined global AI infrastructure in the coming years. Now, as a percentage of total data center revenue, data center sales in China remained well below levels seen on the onset of export controls. Absent any change in regulations, we believe that China's shipments will remain roughly at the current percentage. The market in China for data center solutions remains very competitive. We will continue to comply with export controls while serving our customers.\\n Networking revenue declined 3% sequentially. Our networking attached to GPU compute systems is robust at over 75%. We are transitioning from small NVLink 8 with InfiniBand to large NVLink 72 with Spectrum X. Spectrum X and NVLink Switch revenue increased and represents a major new growth sector. We expect networking to return to growth in Q1. AI requires a new class of networking. NVIDIA offers NVLink Switch systems for scale-up compute. For scale-out, we offer Quantum InfiniBand for HPC supercomputers and Spectrum X for Ethernet environments. Spectrum X enhances the Ethernet for AI computing and has been a huge success. Microsoft Azure, OCI, CoreWeave, and others are building large AI factories.\\n with Spectrum X. The first Stargate data centers will use Spectrum X. Yesterday, Cisco announced integrating Spectrum X into their networking portfolio to help enterprises build AI infrastructure. With its large enterprise footprint and global reach, Cisco will bring NVIDIA Ethernet to every industry. Now moving to gaming and AI PCs. Gaming revenue of $2.5 billion decreased 22% sequentially and 11% year-on-year. Full-year revenue of $11.4 billion increased 9% year-on-year. And demand remained strong throughout the holiday. However, Q4 shipments were impacted by supply constraints. We expect strong sequential growth in Q1 as supply increases. The new GeForce RTX 50 series.\\n desktop and laptop GPUs are here. Built for gamers, creators, and developers, they fuse AI and graphics, redefining visual computing. Powered by the Blackwell architecture, 5th generation Tensor cores, and 4th generation RT cores, and featuring up to 3,400 AI tops. These GPUs deliver a 2x performance leap and new AI driven rendering, including neural shaders, digital human technologies, geometry, and lighting. The new VLSS4 boosts frame rates up to 8x with AI driven frame generation, turning one rendered frame into three. It also features the industry's first real-time application of transformer models, packing 2x more parameters and 4x to compute for unprecedented visual fidelity.\\n We also announced a wave of GeForce Blackwell laptop GPUs with new NVIDIA Max-Q technology that extends battery life by up to an incredible 40%. These laptops will be available starting in March from the world's top manufacturers. Moving to our professional visualization business. Revenue of $511 million was up 5% sequentially and 10% year-on-year. Full-year revenue of $1.9 billion increased 21% year-on-year. Key industry verticals driving demand include automotive and healthcare. NVIDIA technologies and generative AI are reshaping design, engineering, and simulation workloads. Increasingly, these technologies are being leveraged in leading software platforms from ANSYS, Cadence, and Siemens, fueling demand for NVIDIA RTX workstations. Now moving to automotive.\\n Revenue was a record $570 million, up 27% sequentially and up 103% year-on-year. Full-year revenue of $1.7 billion increased 55% year-on-year. Strong growth was driven by the continued ramp in autonomous vehicles, including cars and robo-taxis. At CES, we announced Toyota, the world's largest automaker, will build its next-generation vehicles on NVIDIA Oren, running the safety-certified NVIDIA DRIVE OS. We announced Aurora and Continental will deploy driverless trucks at scale powered by NVIDIA DRIVE 4. Finally, our end-to-end autonomous vehicle platform, NVIDIA DRIVE Hyperion, has passed industry safety assessments by Su Su and Su Ryland, two of the industry's foremost authorities for automotive-grade safety.\\n and cybersecurity. NVIDIA is the first AV platform to receive a comprehensive set of third-party assessments. Okay, moving to the rest of the P&L. Gap gross margins was 73%, and non-gap gross margins was 73.5%, down sequentially as expected with our first deliveries of the Blackwell architecture. As discussed last quarter, Blackwell is a customizable AI infrastructure with several different types of NVIDIA-built chips, multiple networking options, and for air and liquid-cooled data center. We exceeded our expectations in Q4 in ramping Blackwell, increasing system availability, providing several configurations to our customers. As Blackwell ramps, we expect gross margins to be in the low 70s.\\n Initially, we are focused on expediting the manufacturing of Blackwell systems to meet strong customer demand as they race to build out Blackwell infrastructure. When fully ramped, we have many opportunities to improve the cost and gross margin will improve and return to the mid-70s late this fiscal year. Sequentially, GAAP operating expenses were up 9% and non-GAAP operating expenses were 11%, reflecting higher engineering development costs and higher compute and infrastructure costs for new product introductions. In Q4, we returned $8.1 billion to shareholders in the form of share repurchases and cash dividends. Let me turn to the outlook in the first quarter. Federal revenue is expected to be $43 billion, plus or minus 2%.\\n Continuing with its strong demand, we expect a significant ramp of Blackwell in Q1. We expect sequential growth in both data center and gaming. Within data center, we expect sequential growth from both compute and networking. Gap and non-gap gross margins are expected to be 70.6% and 71%, respectively, plus or minus 50 basis points. Gap and non-gap operating expenses are expected to be approximately $5.2 billion and $3.6 billion, respectively. We expect full-year fiscal year 26 operating expenses to grow to be in the mid-30s. Gap and non-gap other incoming expenses are expected to be an income of approximately $400 million. Expecting gains and losses from non-marketable and publicly held equity securities.\\n Gap and non-gap tax rates are expected to be 17 percent, plus or minus 1 percent, excluding any discrete items. Further financial details are included in the CFO commentary and other information available on our IR website, including a new financial information AI agent. In closing, let me highlight upcoming events for the financial community. We will be at the TD Cowen Healthcare Conference in Boston on March 3rd, and at the Morgan Stanley Technology, Media, and Telecom Conference in San Francisco on March 5th. Please join us for our annual GATC conference starting Monday, March 17th, in San Jose, California. Denson will deliver a news-packed keynote on March 18th, and we will host a Q&A session for our financial analysts the next day, March 19th. We look forward to seeing you at these events.\\n call to discuss the results for our first quarter of fiscal 2026 is scheduled for May 28, 2025. We are going to open up the call operator to questions. If you could start that, that would be great. Thank you. At this time, I would like to remind everyone in order to ask a question, please press star then the number one on your telephone keypad. I also ask that you please limit yourself.\\nquestions, please recue. And your first question comes from C.J. Mews with Cantor Fitzgerald. Please go ahead. Yeah, good afternoon. Thank you for taking the question. I guess for me, Judson, as Tefcon compute and reinforcement learning shows such promise, we're clearly seeing an increase in blurring of the lines between training and inference. What does this mean for the potential future of potentially inference-dedicated clusters? How do you think about the overall impact to NVIDIA and your customers? Thank you. Yeah, I appreciate that, C.J. There are now multiple scaling laws. There's the pre-training scaling law, and that's going to continue to scale because we have multimodality. We have data that came from reasoning that are now used to do pre-training. And then the\\n The second is post-training scaling law, using reinforcement learning human feedback, reinforcement learning AI feedback, reinforcement learning verifiable rewards. The amount of computation you use for post-training is actually higher than pre-training. And it's kind of sensible in the sense that you could, while you're using reinforcement learning, generate an enormous amount of synthetic data or synthetically generated tokens. AI models are basically generating tokens to train AI models. And that's post-training. And the third part, this is the part that you mentioned, is test-time compute or reasoning, long thinking, inference scaling. They're all basically the same ideas. And there you have chain of thought, search, the amount of tokens generated.\\n that the amount of inference compute needed is already 100 times more than the one-shot examples and the one-shot capabilities of large language models in the beginning. And that's just the beginning. This is just the beginning. The idea that the next generation could have thousands times, and even hopefully extremely thoughtful and simulation-based and search-based models that could be hundreds of thousands, millions of times more compute than today is in our future. And so the question is, how do you design such an architecture? Some of the models are autoregressive. Some of the models are diffusion-based. Some of the times, you want your data center to have disaggregated.\\n Sometimes it's compacted. And so it's hard to figure out what is the best configuration of a data center, which is the reason why NVIDIA's architecture is so popular. We run every model. We are great at training. The vast majority of our compute today is actually inference. And Blackwell takes all of that to a new level. We designed Blackwell with the idea of reasoning models in mind. And when you look at training, it's many times more performant. But what's really amazing is for long-thinking, test-time scaling, reasoning AI models were tens of times faster, 25 times higher throughput. And so Blackwell is going to be incredible across the board. And when you have a data center that allows you to...\\n configure and use your data center based on are you doing more pre-training now, post-training now, or scaling out your inference, our architecture is fungible and easy to use in all of those different ways. And so we're seeing, in fact, much, much more concentration of a unified architecture than ever before. Your next question comes from the line of Joe Moore with JP Morgan. Please go ahead. Morgan Stanley, actually. Thank you. I wonder if you could talk about GB200. At CES, you sort of talked about the complexity of the rack-level systems and the challenges you have. And then, as you said in the prepared remarks, we've seen a lot of general availability. You know, where are you in terms of that ramp? Are there still bottlenecks to consider at a systems level above and beyond the chip level?\\n Have you maintained your enthusiasm for the NBL 72 platforms? Well, I'm more enthusiastic today than I was at CES. And the reason for that is because we've shipped a lot more since CES. We have some 350 plants manufacturing the one and a half million components that go into each one of the Blackwell racks, race Blackwell racks. Yes, it's extremely complicated and we've successfully and incredibly ramped up Grace Blackwell, delivering some $11 billion in revenues last quarter. We're going to have to continue to scale as demand is quite high and customers are anxious and impatient to get their Blackwell systems. You've probably seen on the web a fair number of celebrations about Grace Blackwell systems coming online. We have them, of course.\\n We have a fairly large installation of gray-spot quilts for our own engineering and our own design teams and software teams. CoreWeave has now been quite public about the successful bring-up of theirs. Microsoft has. Of course, OpenAI has. And you're starting to see many come online. And so I think the answer to your question is nothing is easy about what we're doing. But we're doing great. And all of our partners are doing great. Your next question comes from the line of Vivek Arra with Bank of America Securities. Please go ahead. Thank you for taking my question. If you wouldn't mind confirming if Q1 is the bottom for gross margin. And then my question is for you.\\n forward to give you the confidence that the strong demand can sustain into next year and has DeepSeek and whatever innovation they came up with, has that changed that view in any way? Thank you. Let me first take the first part of the question there regarding the gross margin. During our Blackwell round, our gross margins will be in the low 70s. At this point, we are focusing on expediting our manufacturing, expediting our manufacturing to make sure that we can provide customers as soon as possible. Our Blackwell has fully ramped and once it does, I'm sorry, once our Blackwell fully ramps, we can improve our cost and our gross margin. So we expect to probably be in the mid 70s later this year. You know, walking through what you heard Johnson speak about the systems and their complexity. They are customizable in some cases.\\n They've got multiple networking options, they have liquid-cooled and water-cooled. So we know there is an opportunity for us to improve these gross margins going forward. But right now we are going to focus on getting the manufacturing complete and to our customers as soon as possible. We know several things, Suzette. We have a fairly good line of sight of the amount of capital investment that data centers are building out towards. We know that going forward, the vast majority of software is going to be based on machine learning. And so accelerated computing and generative AI, reasoning AI, are going to be the type of architecture you want in your data center. We have, of course, forecasts and plans from our top partners. And we also know that there are many innovative...\\n really exciting startups that are still coming online as new opportunities for developing the next breakthroughs in AI, whether it's agentic AIs, reasoning AIs, or physical AIs. The number of startups are still quite vibrant and each one of them need a fair amount of computing infrastructure. And so, I think the, whether it's the near-term signals or the mid-term signals, near-term signals of course are, you know, POs and forecasts and things like that. Mid-term signals would be the level of infrastructure and capex scale-out compared to previous years. And then the long-term signals has to do with the fact that we know fundamentally software has changed from hand coding that runs on CPUs to machine learning.\\n and AI-based software that runs on GPUs and accelerated computing systems. And so we have a fairly good sense that this is the future of software. And that maybe, as you roll it out, another way to think about that is we've really only tapped consumer AI and search and some amount of consumer generative AI. Advertising, recommenders, kind of the early days of software. The next wave's coming. Agentic AI for enterprise, physical AI for robotics, and sovereign AI as different regions build out, their AI for their own ecosystems. And so each one of these are fairly off the ground, and we can see them. We can see them because, you know, obviously,\\n We're in the center of much of this development, and we can see great activity happening in all these different places, and these will happen. So near-term, mid-term, long-term. Your next question comes from the line of Harlan Sir with J.P. Morgan. Please go ahead. Yeah, good afternoon. Thanks for taking my question. Your next generation Blackwell Ultra is set to launch in the second half of this year in line with the team's annual product cadence. Jensen, can you help us understand the demand dynamics for Ultra, given that you'll still be ramping the current generation Blackwell solutions? How do your customers and the supply chain also manage the simultaneous ramps of these two products, and is the team still on track to execute Blackwell Ultra in the second half of this year? Yes. Blackwell Ultra is second half. As you know, the first...\\n Blackwell, we had a hiccup that probably cost us a couple of months. We're fully recovered, of course. The team did an amazing job recovering. And all of our supply chain partners and just so many people helped us recover at the speed of light. And so now we've successfully ramped production of Blackwell, but that doesn't stop the next train. The next train is on an annual rhythm and Blackwell Ultra with new networking, new memories, and of course, new processors. And all of that is coming online. We've been working with all of our partners and customers, laying this out. They have all of the necessary information. And we'll work with everybody to do the proper transition. This time between Blackwell and Blackwell Ultra, the system architecture is exactly the same. It's a lot harder going from hopper.\\n to Blackwell because we went from an NVLink 8 system to a NVLink 72 based system. So the chassis, the architecture of the system, the hardware, the power delivery, all of that had to change. This was quite a challenging transition. But the next transition will slot right in. BraceBlock Blackwell Ultra will slot right in. We've also already revealed and been working very closely with all of our partners on the click after that. And the click after that is called Vera Rubin. And all of our partners are getting up to speed on the transition of that. And so preparing for that transition. And again, we're gonna provide a big, big, huge step up. And so come to GTC and I'll talk to you about Blackwell Ultra, Vera Rubin, and then show you what's the one click after that. Really, really.\\n new products, so come to GTC, please. Your next question comes from the line of Timothy Arcuri with UBS. Please go ahead. Thanks a lot. Jensen, we hear a lot about custom ASICs. Can you kind of speak to the balance between custom ASIC and merchant GPU? We hear about some of these heterogeneous superclusters to use both GPU and ASIC.\""
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are to generate a summary report based on the formats and contents of an example.\n",
    "Report quantitative items when they are available.\n",
    "\n",
    "\n",
    "Example:\n",
    "{example}\n",
    "\n",
    "===\n",
    "Text to summarize:\n",
    "\n",
    "{text}\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "result = llm.invoke(prompt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "1. 結論：\n",
      "● FY25Q4營收393億美元，季增12%/年增78%，超出了375億美元的預期。全年營收達到1305億美元，年增114%。數據中心營收為1152億美元，翻了一番以上。Blackwell銷售額超出預期，達到110億美元，成為公司歷史上最快的新產品放量速度。展望未來，預計FY26Q1營收為430億美元，上下浮動2%。Blackwell將在Q1顯著增長，數據中心和遊戲業務預計將實現季度增長。\n",
      "\n",
      "2. 重點摘要：\n",
      "● FY25Q4數據中心營收達到創紀錄的356億美元，季增16%/年增93%。Blackwell銷售額達到110億美元，超出預期，標誌著公司最快的新產品放量速度。\n",
      "● FY25Q4遊戲業務營收為25億美元，季減22%/年減11%，受供應限制影響，但預計Q1將因供應增加而強勁增長。\n",
      "● FY25Q4汽車業務營收達到創紀錄的5.7億美元，季增27%/年增103%，由自動駕駛汽車的持續增長所驅動。\n",
      "● 預計FY26Q1營收為430億美元，上下浮動2%。數據中心和遊戲業務預計將實現季度增長，Blackwell將顯著增長。\n",
      "● 預計毛利率在Blackwell放量初期將在70%左右的低段，隨著產能提升，預計在本財年晚些時候回到70%中段。\n",
      "\n",
      "3. 法說內容：\n",
      "(1) 現況分析\n",
      "● FY25Q4營收393億美元，季增12%/年增78%，超出預期。全年營收1305億美元，年增114%。\n",
      "● FY25數據中心營收1152億美元，比去年翻了一番以上。FY25Q4數據中心營收356億美元，季增16%/年增93%。Blackwell銷售額超出預期，達到110億美元，是公司歷史上最快的新產品放量。\n",
      "● 大型CSP佔數據中心收入的一半左右，銷售額同比增長近2倍。\n",
      "● FY25Q4遊戲業務營收25億美元，季減22%/年減11%，全年營收114億美元，年增9%。Q4出貨量受到供應限制的影響。\n",
      "● FY25Q4專業可視化業務收入5.11億美元，季增5%/年增10%，全年收入19億美元，年增21%。\n",
      "● FY25Q4汽車業務收入創紀錄的5.7億美元，季增27%/年增103%，全年收入17億美元，年增55%。\n",
      "● FY25Q4 GAAP毛利率為73%，Non-GAAP毛利率為73.5%，環比下降，符合預期，因為Blackwell架構開始交付。\n",
      "● FY25Q4 GAAP運營費用環比增長9%，Non-GAAP運營費用環比增長11%。\n",
      "\n",
      "(2) 未來展望\n",
      "● 預計FY26Q1營收為430億美元，上下浮動2%。\n",
      "● 預計FY26Q1數據中心和遊戲業務將實現季度增長，數據中心業務的計算和網絡業務也將實現季度增長。Blackwell預計在Q1將顯著增長。\n",
      "● 預計毛利率在Blackwell放量初期將在70%左右的低段，Non-GAAP毛利率預計為71%，上下浮動50個基點。預計在本財年晚些時候毛利率將回到70%中段。\n",
      "● 預計FY26全年運營費用將增長到30%中段。\n",
      "● 預計FY26Q1 GAAP和Non-GAAP運營費用分別約為52億美元和36億美元。\n",
      "● 預計FY26Q1 GAAP和Non-GAAP稅率均為17%，上下浮動1%。\n",
      "● 遊戲業務預計Q1將因供應增加而強勁增長。新的GeForce RTX 50系列桌面和筆記本GPU即將上市。\n",
      "● 汽車垂直領域的收入預計在本財年將增長到約50億美元。\n",
      "\n",
      "(3) 公司營運\n",
      "● Blackwell銷售額超出預期，達到110億美元，是公司歷史上最快的新產品放量速度。Blackwell產量已全面啟動，並正在快速增加供應。\n",
      "● 大型CSP佔數據中心收入的一半左右，銷售額同比增長近2倍。區域雲託管和視頻GPU佔數據中心收入的百分比有所增加。\n",
      "● 消費互聯網收入同比增長3倍，受到生成式AI和深度學習用例的擴展推動。\n",
      "● 網絡收入環比下降3%，但與GPU計算系統相關的網絡收入仍然強勁，超過75%。預計網絡業務將在Q1恢復增長。Spectrum X和NVLink Switch收入增加，代表著一個主要的新增長領域。\n",
      "● 中國的數據中心銷售額佔數據中心總收入的百分比仍然遠低於出口管制開始時的水平，預計在中國的發貨量將大致保持在目前的百分比。中國的數據中心解決方案市場仍然競爭非常激烈。\n",
      "● NVIDIA提供了用於擴展計算的NVLink Switch系統，以及用於橫向擴展的Quantum InfiniBand和Spectrum X。Spectrum X增強了以太網的AI計算能力，並已獲得巨大成功。\n",
      "● NVIDIA的汽車垂直領域收入預計在本財年將增長到約50億美元。豐田汽車將在其下一代汽車上採用NVIDIA Orin，Aurora和Continental將大規模部署由NVIDIA DRIVE 4驅動的無人駕駛卡車。\n",
      "● NVIDIA DRIVE Hyperion是首個獲得第三方全面評估的AV平台。\n",
      "● NVIDIA在過去兩年中將推理成本降低了200倍。\n"
     ]
    }
   ],
   "source": [
    "print(result[\"content\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python311",
   "language": "python",
   "display_name": "Python 3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
