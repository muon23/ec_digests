 Is that something customers are planning on building, or will these infrastructures remain fairly distinct? Well, we build very different things than ASICs in some ways, completely different in some areas we intersect. We're different in several ways. One, NVIDIA's architecture is general. You know, whether you've optimized for autoregressive models or diffusion-based models or vision-based models or multimodal models or text models, we're great at all of it. We're great at all of it because our software stack is so...
 our architecture's flexible, our software stack's ecosystem is so rich that we're the initial target of most exciting innovations and algorithms. And so by definition, we're much, much more general than narrow. We're also really good from the end to end, from data processing, the curation of the training data, to the training of the data, of course, to reinforcement learning used in post-training, all the way to inference with test-time scaling. So we're general, we're end-to-end, and we're everywhere. And because we're not in just one cloud, we're in every cloud, we could be on-prem, we could be in a robot. Our architecture is much more accessible and a great target, initial target, for anybody who's starting up.
 a new company, and so we're everywhere. And then the third thing I would say is that our performance and our rhythm is so incredibly fast. Remember that these data centers are always fixed in size. They're fixed in size or they're fixed in power. And if our performance per watt is anywhere from 2x to 4x to 8x, which is not unusual, it translates directly to revenues. And so if you have a 100-megawatt data center, if the performance or the throughput in that 100-megawatt or that gigawatt data center is 4 times or 8 times higher, your revenues for that gigawatt data center is 8 times higher. And the reason that is so different than data centers of the past is because AI factories are directly monetizable through its tokens generated. And so the token throughput.
 of our architecture being so incredibly fast is just incredibly valuable to all of the companies that are building these things for revenue generation reasons and capturing the fast ROIs. And so I think the third reason is performance. And then the last thing that I would say is the software stack is incredibly hard. Building an ASIC is no different than what we do. We have to build a new architecture. And the ecosystem that sits on top of our architecture is 10 times more complex today than it was two years ago. And that's fairly obvious, because the amount of software that the world is building on top of our architecture is growing exponentially, and AI is advancing very quickly. So bringing that whole ecosystem on top of multiple chips is hard. And so I would say that those four reasons, and then finally, I will say this.
 Just because a chip is designed doesn't mean it gets deployed. And you've seen this over and over again. There are a lot of chips that get built. But when the time comes, a business decision has to be made. And that business decision is about deploying a new engine, a new processor, into a limited AI factory in size, in power, and in time. And our technology is not only more advanced, more performant, it has much, much better software capability. And very importantly, our ability to deploy is lightning fast. And so these things are not for the faint of heart, as everybody knows now. And so there's a lot of different reasons why we do well, why we win.
 We have a line of Ben Reitz with Malias Research. Please go ahead. Yeah, hi. Ben Reitz is here. Hey, thanks a lot for the question. Hey, Jensen, it's a geography-related question. You know, you did a great job explaining some of the demand underlying factors here on the strength, but U.S. was up about $5 billion or so sequentially, and I think, you know, there is a concern about whether U.S. can pick up the slack if there's regulations towards other geographies. And I was just wondering, as we go throughout the year, you know, if this kind of surge in the U.S. continues and it's going to be â€“ whether that's okay, and if that, you know, underlies your growth rate, how can you keep growing so fast with this mixed shift towards the U.S.? Your guidance looks like China is probably up sequentially. So just wondering if you could go through that dynamic and maybe collect and weigh in. Thanks a lot.
 China is approximately the same percentage as Q4 and as previous quarters. It's about half of what it was before the export control, but it's approximately the same percentage. With respect to geographies, the takeaway is that AI is software. It's modern software. It's incredible modern software, but it's modern software. And AI has gone mainstream. AI is used in delivery services everywhere, shopping services everywhere. If you were to buy a quarter of milk and it's delivered to you, AI was involved. And so almost everything that a consumer service provides, AI is at the core of it.
 A student will use AI as a tutor. Health care services use AI. Financial services use AI. No fintech company will not use AI. Every fintech company will. Climate tech company use AI. Mineral discovery now uses AI. The number of every higher education, every university uses AI. And so I think it is fairly safe to say that AI has gone mainstream and that it's being integrated into every application. And our hope is that, of course, the technology continues to advance safely and advance in a helpful way to our society. And with that, I do believe that we're at the beginning of this new transition. And what I mean by that in the
 beginning is remember behind us has been decades of data centers and decades of computers that have been built and they've been built for a world of hand-coding and general-purpose computing and CPUs and so on so forth. And going forward I think it's fairly safe to say that that world is going to be almost all software be infused with AI. All software and all services will be based on ultimately based on machine learning. The data flywheel is going to be part of improving software and services and that the future of computers will be accelerated. The future computers will be based on AI. And we're really two years into that journey and in modernizing computers that have taken decades to build out. So I'm fairly sure that we're in the beginning of this new era. And then lastly in no
 technology has ever had the opportunity to address a larger part of the world's GDP than AI. No software tool ever has. And so this is now a software tool that can address a much larger part of the world's GDP more than any time in history. And so the way we think about growth and the way we think about whether something is big or small has to be in the context of that. And when you take a step back and look at it from that perspective, we're really just in the beginnings. Your next question comes from the line of Aaron Rakers with Wells Fargo. Please go ahead. Aaron, your line is open. Your next question comes from Mark Lupas with Evercore ISI.
 Hi, this is Martin Papas, thanks for taking the question. I had a clarification and a question, Colette, for the clarification. Did you say that enterprise within the data center grew 2X year-on-year for the January quarter? And if so, would that make it faster going than the hyperscalers? And then, Jensen, for you, the question, hyperscalers are the biggest purchasers of your solutions, but they buy equipment for both internal and external workloads, external workloads being cloud services that enterprises use. So the question is, can you give us a sense of how that hyperscaler spend splits between that external workload and internal? And as these new AI workloads and applications come up, would you expect enterprises to become a larger part of that consumption mix and does that impact how you develop your...
 service your ecosystem. Thank you. Sure. Thanks for the question regarding our enterprise business. Yes, it grew to X and very similar to what we were seeing with our large CSPs. Keep in mind, these are both important areas to understand. Working with the CSPs can be working on large language models, can be working on inference in their own work, but keep in mind, this is also where the enterprises are surfacing. Your enterprises are both with your CSPs as well as in terms of building on their own. They're both, correct, growing quite well. The CSPs are about half of our business. And the CSPs have internal consumption and external consumption, as you say. And we're using, of course, use for internal consumption. We work very closely with...
 with all of them to optimize workloads that are internal to them, because they have a large infrastructure of NVIDIA gear that they could take advantage of. And the fact that we could be used for AI on the one hand, video processing on the other hand, data processing like Spark, we're fungible. And so the useful life of our, of our infrastructure is much better.