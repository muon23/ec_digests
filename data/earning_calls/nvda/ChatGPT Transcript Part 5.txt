 If the useful life is much longer, then the TCO is also lower. And so the second part is, how do we see the growth of enterprise or not CSPs, if you will, going forward? And the answer is, I believe long-term, it is by far larger. And the reason for that is because if you look at the
 industry today and what is not served by the computer industry is largely industrial. So let me give you an example. When we say enterprise, and let's say let's use a car company as an example because they make both soft things and hard things. And so in the case of a car company, the employees would be what we call enterprise and agentic AI and software planning systems and tools and we have some really exciting things to share with you guys at GTC. Those agentic systems are for employees to make employees more productive, to design, to market, to plan, to operate their company. That's agentic AI. On the other hand, the cars that they manufacture also need AI. They need an AI system that trains the cars, treats this entire giant fleet of cars. And you know today there's a billion cars on the road today.
 on the road, someday there'll be a billion cars on the road and every single one of those cars will be robotic cars. And they'll all be collecting data and we'll be improving them using an AI factory. Whereas they have a car factory today, in the future they'll have a car factory and an AI factory. And then inside the car itself is a robotic system. And so as you can see, there are three computers involved. And there's the computer that helps the people. There's the computer that builds the AI for the machineries. It could be, of course, it could be a tractor. It could be a lawnmower. It could be a human or a robot that's being developed today. It could be a building. It could be a warehouse. These physical systems require a new type of AI we call physical AI. They can't just understand the meaning of words and languages, but they have to understand the meaning of the world. Friction and inertia, object permanence, and cause and effect.
 and all of those type of things that are common sense to you and I, but AI has to go learn those physical effects. So we call that physical AI. That whole part of using agentic AI to revolutionize the way we work inside companies, that's just starting. This is now the beginning of the agentic AI era, and you hear a lot of people talking about it, and we've got some really great things going on. And then there's the physical AI after that, and then there are robotic systems after that. And so these three computers are all brand new, and my sense is that long-term, this will be by far the larger of them all, which kind of makes sense. You know, the world's GDP is represented by either heavy industries or industrials and companies that are providing for those. Thank you.
 with Wells Fargo, please go ahead. Yeah, thanks for letting me back in. Jensen, I'm curious as we now approach the two-year anniversary of really the hopper inflection that you saw in 2023 and in Gen AI in general, and we think about the roadmap you have in front of us, how do you think about the infrastructure that's been deployed from a replacement cycle perspective and whether, you know, if it's, you know, GB300 or if it's the Rubin cycle where we start to see maybe some refresh opportunity? I'm just curious to how you look at that. Yeah, I appreciate it. First of all, people are still using Voltas and Pascals and Amperes. And the reason for that is because there are always things that, because CUDA is so programmable, you could use it, right? Well, one of the major use cases right now is data processing and data curation. You find a circumstance?
 that an AI model is not very good at. You present that circumstance to a vision language model, let's say. Let's say it's a car. You present that circumstance to a vision language model. The vision language model actually looks at the circumstances and says, this is what happened, and I wasn't very good at it. You then take that response, the prompt, and you go and prompt an AI model to go find in your whole lake of data other circumstances like that, whatever that circumstance was. And then you use an AI to do domain randomization and generate a whole bunch of other examples. And then from that, you can go train the model. And so you could use the Amperes to go and do data processing and data curation and machine learning-based search. And then you create the training.
 training data set, which you then present to your Hopper systems for training. And so each one of these architectures are completely, you know, they're all CUDA compatible, and so everything runs on everything. But if you have infrastructure in place, then you can put the less intensive workloads onto the install base of the All of our CPUs are very well deployed. We have time for one more question, and that question comes from Atif Malik with Citi. Please go ahead. Hi, thank you for taking my question. I have a follow-up question, gross margins for Colette. Colette, I understand there are many moving parts, the Backwell Yields and Relink 72 and Ethernet mix, and you kind of tiptoed the earlier question if April quarter is the bottom, but second half would have to ramp like 200 basis points per quarter to get to the mid-70s.
 range that you're giving for the end of the fiscal year, and we still don't know much about tariffs' impact to broader semiconductors. So what kind of gives you the confidence in that trajectory in the back half of this year? Yeah. Thanks for the question. Our gross margins, they're quite complex in terms of the material and everything that we've put together in a Blackwell system. Tremendous amount of opportunity to look at a lot of different pieces of that on how we can better improve our gross margins over time. Remember, we have many different configurations as well on Blackwell that will be able to help us do that. So together, working after we get some of these really strong ramping completed for our customers, we can begin a lot of that work. If not, we're going to probably start as soon as possible if we can, and if we can improve it in the short term
 we will also do that. Tariffs, at this point, it's a little bit of an unknown. It's an unknown until we understand further what the U.S. government's plan is, both its timing, its where, and how much. So, at this time, we are awaiting. But, again, we would, of course, always follow export controls and or tariffs in that manner. Ladies and gentlemen, that does conclude our question-and-answer session. I'm sorry. Thank you. No, no. We're going to open up to Jensen. I just wanted to thank you. I just wanted to thank you. Thank you, Collette. The demand for Blackwell is extraordinary. AI is evolving beyond perception and generative AI into reasoning. With reasoning AI, we're observing another scaling law. In first...
 time or test time scaling, the more computation, the more the model thinks, the smarter the answer. Models like OpenAI, ROC3, DeepSeq R1 are reasoning models that apply inference time scaling. Reasoning models can consume 100 times more compute. Future reasoning models can consume much more compute. DeepSeq R1 has ignited global enthusiasm. It's an excellent innovation. But even more importantly, it has open-sourced a world-class reasoning AI model. Nearly every AI developer is applying R1, or chain of thought and reinforcement learning techniques like R1, to scale their model's performance. We now have three scaling laws, as I mentioned earlier, driving the demand for AI computing. The traditional scaling laws of AI remains intact.
 Foundation models are being enhanced with multi-modality, and pre-training is still growing. But it's no longer enough. We have two additional scaling dimensions. Post-training scaling, where reinforcement learning, fine-tuning, model distillation, require orders of magnitude more compute than pre-training alone. Inference time scaling and reasoning, where a single query can demand 100 times more compute. We designed Blackwell for this moment, a single platform that can easily transition from pre-training, post-training, and test-time scaling. Blackwell's FP4 transformer engine and NVLink72 scale-up fabric and new software technologies let Blackwell process reasoning AI models 25 times faster than Hopper.
 Blackwell, in all of its configurations, is in full production. Each Grace Blackwell NVLink72 rack is an engineering marvel. One and a half million components produced across 350 manufacturing sites by nearly 100,000 factory operators. AI is advancing at light speed. We're at the beginning of reasoning AI and inference time scaling. But we're just at the start of the age of AI. Multimodal AI, enterprise AI, sovereign AI, and physical AI are right around the corner. We will grow strongly in 2025. Going forward, data centers will dedicate most of CapEx to accelerated computing and AI. Data centers will increasingly become AI factories. And every company will have either rented or self-operated.
 I want to thank all of you for joining us today. Come join us at GTC in a couple of weeks.