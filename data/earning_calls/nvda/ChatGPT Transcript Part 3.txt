 This is just the beginning. The idea that the next generation could have thousands times, and even hopefully extremely thoughtful and simulation-based and search-based models that could be hundreds of thousands, millions of times more compute than today is in our future. And so the question is, how do you design such an architecture? Some of the models are autoregressive. Some of the models are diffusion-based. Some of the times, you want your data center to have disaggregated.
 Sometimes it's compacted. And so it's hard to figure out what is the best configuration of a data center, which is the reason why NVIDIA's architecture is so popular. We run every model. We are great at training. The vast majority of our compute today is actually inference. And Blackwell takes all of that to a new level. We designed Blackwell with the idea of reasoning models in mind. And when you look at training, it's many times more performant. But what's really amazing is for long-thinking, test-time scaling, reasoning AI models were tens of times faster, 25 times higher throughput. And so Blackwell is going to be incredible across the board. And when you have a data center that allows you to...
 configure and use your data center based on are you doing more pre-training now, post-training now, or scaling out your inference, our architecture is fungible and easy to use in all of those different ways. And so we're seeing, in fact, much, much more concentration of a unified architecture than ever before. Your next question comes from the line of Joe Moore with JP Morgan. Please go ahead. Morgan Stanley, actually. Thank you. I wonder if you could talk about GB200. At CES, you sort of talked about the complexity of the rack-level systems and the challenges you have. And then, as you said in the prepared remarks, we've seen a lot of general availability. You know, where are you in terms of that ramp? Are there still bottlenecks to consider at a systems level above and beyond the chip level?
 Have you maintained your enthusiasm for the NBL 72 platforms? Well, I'm more enthusiastic today than I was at CES. And the reason for that is because we've shipped a lot more since CES. We have some 350 plants manufacturing the one and a half million components that go into each one of the Blackwell racks, race Blackwell racks. Yes, it's extremely complicated and we've successfully and incredibly ramped up Grace Blackwell, delivering some $11 billion in revenues last quarter. We're going to have to continue to scale as demand is quite high and customers are anxious and impatient to get their Blackwell systems. You've probably seen on the web a fair number of celebrations about Grace Blackwell systems coming online. We have them, of course.
 We have a fairly large installation of gray-spot quilts for our own engineering and our own design teams and software teams. CoreWeave has now been quite public about the successful bring-up of theirs. Microsoft has. Of course, OpenAI has. And you're starting to see many come online. And so I think the answer to your question is nothing is easy about what we're doing. But we're doing great. And all of our partners are doing great. Your next question comes from the line of Vivek Arra with Bank of America Securities. Please go ahead. Thank you for taking my question. If you wouldn't mind confirming if Q1 is the bottom for gross margin. And then my question is for you.
 forward to give you the confidence that the strong demand can sustain into next year and has DeepSeek and whatever innovation they came up with, has that changed that view in any way? Thank you. Let me first take the first part of the question there regarding the gross margin. During our Blackwell round, our gross margins will be in the low 70s. At this point, we are focusing on expediting our manufacturing, expediting our manufacturing to make sure that we can provide customers as soon as possible. Our Blackwell has fully ramped and once it does, I'm sorry, once our Blackwell fully ramps, we can improve our cost and our gross margin. So we expect to probably be in the mid 70s later this year. You know, walking through what you heard Johnson speak about the systems and their complexity. They are customizable in some cases.
 They've got multiple networking options, they have liquid-cooled and water-cooled. So we know there is an opportunity for us to improve these gross margins going forward. But right now we are going to focus on getting the manufacturing complete and to our customers as soon as possible. We know several things, Suzette. We have a fairly good line of sight of the amount of capital investment that data centers are building out towards. We know that going forward, the vast majority of software is going to be based on machine learning. And so accelerated computing and generative AI, reasoning AI, are going to be the type of architecture you want in your data center. We have, of course, forecasts and plans from our top partners. And we also know that there are many innovative...
 really exciting startups that are still coming online as new opportunities for developing the next breakthroughs in AI, whether it's agentic AIs, reasoning AIs, or physical AIs. The number of startups are still quite vibrant and each one of them need a fair amount of computing infrastructure. And so, I think the, whether it's the near-term signals or the mid-term signals, near-term signals of course are, you know, POs and forecasts and things like that. Mid-term signals would be the level of infrastructure and capex scale-out compared to previous years. And then the long-term signals has to do with the fact that we know fundamentally software has changed from hand coding that runs on CPUs to machine learning.
 and AI-based software that runs on GPUs and accelerated computing systems. And so we have a fairly good sense that this is the future of software. And that maybe, as you roll it out, another way to think about that is we've really only tapped consumer AI and search and some amount of consumer generative AI. Advertising, recommenders, kind of the early days of software. The next wave's coming. Agentic AI for enterprise, physical AI for robotics, and sovereign AI as different regions build out, their AI for their own ecosystems. And so each one of these are fairly off the ground, and we can see them. We can see them because, you know, obviously,
 We're in the center of much of this development, and we can see great activity happening in all these different places, and these will happen. So near-term, mid-term, long-term. Your next question comes from the line of Harlan Sir with J.P. Morgan. Please go ahead. Yeah, good afternoon. Thanks for taking my question. Your next generation Blackwell Ultra is set to launch in the second half of this year in line with the team's annual product cadence. Jensen, can you help us understand the demand dynamics for Ultra, given that you'll still be ramping the current generation Blackwell solutions? How do your customers and the supply chain also manage the simultaneous ramps of these two products, and is the team still on track to execute Blackwell Ultra in the second half of this year? Yes. Blackwell Ultra is second half. As you know, the first...
 Blackwell, we had a hiccup that probably cost us a couple of months. We're fully recovered, of course. The team did an amazing job recovering. And all of our supply chain partners and just so many people helped us recover at the speed of light. And so now we've successfully ramped production of Blackwell, but that doesn't stop the next train. The next train is on an annual rhythm and Blackwell Ultra with new networking, new memories, and of course, new processors. And all of that is coming online. We've been working with all of our partners and customers, laying this out. They have all of the necessary information. And we'll work with everybody to do the proper transition. This time between Blackwell and Blackwell Ultra, the system architecture is exactly the same. It's a lot harder going from hopper.
 to Blackwell because we went from an NVLink 8 system to a NVLink 72 based system. So the chassis, the architecture of the system, the hardware, the power delivery, all of that had to change. This was quite a challenging transition. But the next transition will slot right in. BraceBlock Blackwell Ultra will slot right in. We've also already revealed and been working very closely with all of our partners on the click after that. And the click after that is called Vera Rubin. And all of our partners are getting up to speed on the transition of that. And so preparing for that transition. And again, we're gonna provide a big, big, huge step up. And so come to GTC and I'll talk to you about Blackwell Ultra, Vera Rubin, and then show you what's the one click after that. Really, really.
 new products, so come to GTC, please. Your next question comes from the line of Timothy Arcuri with UBS. Please go ahead. Thanks a lot. Jensen, we hear a lot about custom ASICs. Can you kind of speak to the balance between custom ASIC and merchant GPU? We hear about some of these heterogeneous superclusters to use both GPU and ASIC.