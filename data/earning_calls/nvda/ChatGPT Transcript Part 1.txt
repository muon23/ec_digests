have been placed on mute to prevent any background noise. After the speaker's remarks, there will be a question and answer session. If you would like to ask a question during this time, simply press star followed by the number one on your telephone keypad. And if you'd like to withdraw your question, again, press star one. Thank you. Stuart Stecker, you may begin your conference. Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the fourth quarter of fiscal 2025. With me today from NVIDIA are Jensen Wong, President and Chief Executive Officer, and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the first quarter of fiscal 2026. The content of today's call is NVIDIA's property.
 it can't be reproduced or transcribed without prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release. Our most recent forms, 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made, as of today, February 26, 2025, based on information currently available to us, except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures and find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on
 on our website. With that, let me turn the call over to Collette. Thanks, Stuart. Q4 was another record quarter. Revenue of 39.3 billion was up 12% sequentially and up 78% year on year. And above our outlook of 37.5 billion. For fiscal 2025, revenue was 130.5 billion, up 114% in the prior year. Let's start with data center. Data center revenue for fiscal 2025 was 115.2 billion, more than doubling from the prior year. In the fourth quarter, data center revenue of 35.6 billion was a record, up 16% sequentially and 93% year on year. As the Blackwell ramp commenced at Hopper 200 continued sequential growth.
 In Q4, Blackwell sales exceeded our expectations. We delivered $11 billion of Blackwell revenue to meet strong demand. This is the fastest product ramp in our company's history, unprecedented in its speed and scale. Blackwell production is in full gear across multiple configurations, and we are increasing supply quickly, expanding customer adoption. Our Q4 data center compute revenue jumped 18% sequentially and over 2x year on year. Customers are racing to scale infrastructure to train the next generation of cutting-edge models and unlock the next level of AI capabilities. With Blackwell, it will be common for these clusters to start with 100,000 GPUs or more. Shipments have already started for multiple infrastructures of this size.
 Post-training and model customization are fueling demand for NVIDIA infrastructure and software as developers and enterprises leverage techniques such as fine-tuning, reinforcement learning, and distillation to tailor models for domain-specific use cases. Hugging Face alone hosts over 90,000 derivatives graded from the Lama Foundation model. The scale of post-training and model customization is massive and can collectively demand orders of magnitude more compute than pre-training. Our inference demand is accelerating, driven by test-time scaling and new reasoning models like OpenAI's O3, DeepSeq R1, and Grok3. Long-thinking reasoning AI can require 100x more compute per task compared to one shot.
 inferences. Blackwell was architected for reasoning AI inference. Blackwell supercharges reasoning AI models with up to 25X higher token throughput and 20X lower cost versus Hopper 100. It is revolutionary. Transformer Engine is built for LLM and mixture of experts inference. And its NVLink domain delivers 14X the throughput of PCIe Gen 5, ensuring the response time, throughput, and cost efficiency needed to tackle the growing complexity of inference of scale. Companies across industries are tapping into NVIDIA's full stack inference platform to boost performance and slash costs. Now tripled inference throughput and cut costs by 66% using NVIDIA TensorRT.
 for its screenshot feature. Perplexity sees 435 million monthly queries and reduced its inference costs 3x with NVIDIA Triton Inference Server and TensorRT LLM. Microsoft Bing achieved a 5x speedup and major TCO savings for visual search across billions of images with NVIDIA TensorRT and acceleration libraries. Blackwell has great demand for inference. Many of the early GB200 deployments are earmarked for inference, a first for a new architecture. Blackwell addresses the entire AI market from pre-training, post-training, to inference across clouds, to on-premise, to enterprise. Sudha's programmable architecture accelerates every AI model and over 4,400 applications, ensuring large...
 infrastructure investments against obsolescence in rapidly evolving markets. Our performance and pace of innovation is unmatched. We're driven to a 200x reduction in inference cost in just the last two years. We deliver the lowest TCO and the highest ROI. And full-stack optimizations for NVIDIA and our large ecosystem, including 5.9 million developers, continuously improve our customers' economics. In Q4, large CSPs represented about half of our data center revenue, and these sales increased nearly 2x year-on-year. Large CSPs were some of the first to stand up Blackwell with Azure, GCP, AWS, and OCI, bringing GV200 systems to cloud regions around the world to meet...
 surging cluster demand for AI. Regional cloud hosting and video GPUs increased as a percentage of data center revenue, reflecting continued AI factory build-outs globally and rapidly rising demand for AI reasoning models and agents. Or we've launched a 100,000 GB 200 cluster-based instance with NVLink Switch and Quantum2 InfiniBand. Consumer Internet Revenue Group 3X year on year, driven by an expanding set of generative AI and deep learning use cases. These include recommender systems, vision language understanding, synthetic data generation search and agentic AI. For example, XAI is adopting the GB 200 to train and inference its next generation of grog AI models. Meta's cutting-edge and dry.
 Adromeda advertising engine runs on NVIDIA's Grace Hopper superchip, serving vast quantities of ads across Instagram, Facebook applications. Adromeda harnesses Grace Hopper's fast interconnect and large memory to boost inference throughput by 3x, enhance ad personalization, and deliver meaningful jumps in monetization and ROI. Increase nearly 2x year-on-accelerated demand for model fine-tuning, RAG, and agentic AI workflows, and GPU-accelerated data processing. We introduced NVIDIA LLAMA Numitron model family NIMS to help developers create and deploy AI agents across a range of applications, including customer support, fraud detection, product supply chain, and inventory management.
 AI agent platform providers, including SAP and ServiceNow, are among the first to use new models. Healthcare leaders IQVIA, Illumina, and Mayo Clinic are well, as AHRQ Institute, are using NVIDIA AI to speed drug discovery, enhance genomic research, and pioneer advanced healthcare services with generative and agentic AI. As AI expands beyond the digital world, NVIDIA infrastructure and software platforms are increasingly being adopted to power robotics and physical AI development. One of the early and largest robotics applications in autonomous vehicles, where virtually every AV company is developing on NVIDIA in the data center, the car, or both. NVIDIA's automotive vertical revenue is expected to grow to approximately five billion this fiscal year.
 At CES, Hyundai Motor Group announced it is adopting NVIDIA technologies to accelerate AV and robotics development at smart factory initiatives. Vision transformers, self-supervised learning, multimodal sensor fusion, and high-fidelity simulation are driving breakthroughs in AV development and will require 10X more compute. At CEX, we announced the NVIDIA COSMOS World Foundation Model Platform. Just as language foundation models have revolutionized language AI, COSMOS is a physical AI to revolutionize robotics. Medium robotics and automotive companies, including ride-sharing giant Uber, are among the first to adopt the platform. From a geographic perspective, sequential growth in our data center revenue was strongest in the U.S.
 driven by the initial ramp of Blackwell. Countries across the globe are building their AI ecosystems and demand for compute infrastructure is surging. France's 200 billion euro AI investment and the EU's 200 billion euro invest AI initiatives offer a glimpse into the build out to set redefined global AI infrastructure in the coming years.